{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib as pkl\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import     Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['NATURAL GAS_Percent_Change', 'GOLD_Percent_Change',\n",
    "       'WTI CRUDE_Percent_Change', 'BRENT CRUDE_Percent_Change',\n",
    "       'SOYBEANS_Percent_Change', 'CORN_Percent_Change',\n",
    "       'COPPER_Percent_Change', 'ALUMINIUM_Percent_Change',\n",
    "       'ZINC_Percent_Change', 'NICKEL_Percent_Change', 'WHEAT_Percent_Change',\n",
    "       'SUGAR_Percent_Change', 'COFFEE_Percent_Change',\n",
    "       'COTTON_Percent_Change'])\n",
    "\n",
    "y = df[['NATURAL GAS_Percent_Change', 'GOLD_Percent_Change',\n",
    "       'WTI CRUDE_Percent_Change', 'BRENT CRUDE_Percent_Change',\n",
    "       'SOYBEANS_Percent_Change', 'CORN_Percent_Change',\n",
    "       'COPPER_Percent_Change', 'ALUMINIUM_Percent_Change',\n",
    "       'ZINC_Percent_Change', 'NICKEL_Percent_Change', 'WHEAT_Percent_Change',\n",
    "       'SUGAR_Percent_Change', 'COFFEE_Percent_Change',\n",
    "       'COTTON_Percent_Change']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\n",
    "    'Model',\n",
    "    'Mean Squared Error',\n",
    "    'Root Mean Squared Error',\n",
    "    'Mean Absolute Error',\n",
    "    'R-squared'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model tranning\n",
    "model = MultiOutputRegressor(SVR())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "with open(f\"./models/svm.pkl\", \"wb\") as file:\n",
    "    pkl.dump(model, file)\n",
    "\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[0, 'Model'] = 'svm'\n",
    "results_df.loc[0, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[0, 'Mean Squared Error'] = mse\n",
    "results_df.loc[0, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[0, 'R-squared'] = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor model tranning\n",
    "model = MultiOutputRegressor(DecisionTreeRegressor())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "with open(f\"./models/DecisionTreeRegressor.pkl\", \"wb\") as file:\n",
    "    pkl.dump(model, file)\n",
    "\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[1, 'Model'] = 'DecisionTreeRegressor'\n",
    "results_df.loc[1, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[1, 'Mean Squared Error'] = mse\n",
    "results_df.loc[1, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[1, 'R-squared'] = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsRegressor model tranning\n",
    "model = MultiOutputRegressor(KNeighborsRegressor())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "with open(f\"./models/KNeighborsRegressor.pkl\", \"wb\") as file:\n",
    "    pkl.dump(model, file)\n",
    "\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[2, 'Model'] = 'KNeighborsRegressor'\n",
    "results_df.loc[2, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[2, 'Mean Squared Error'] = mse\n",
    "results_df.loc[2, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[2, 'R-squared'] = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ML_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df =pd.read_csv('ML_model_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression model tranning\n",
    "model = MultiOutputRegressor(LinearRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "with open(f\"./models/LinearRegression.pkl\", \"wb\") as file:\n",
    "    pkl.dump(model, file)\n",
    "\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[3, 'Model'] = 'LinearRegression'\n",
    "results_df.loc[3, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[3, 'Mean Squared Error'] = mse\n",
    "results_df.loc[3, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[3, 'R-squared'] = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ML_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('ML_model_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the VAR model using only y_train\n",
    "model = VAR(y_train)\n",
    "model_fitted = model.fit()\n",
    "\n",
    "# Forecast future values\n",
    "forecasted_values = model_fitted.forecast(y_train.values[-model_fitted.k_ar:], steps=len(y_test))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, forecasted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, forecasted_values)\n",
    "r2 = r2_score(y_test, forecasted_values)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "#save results\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[4, 'Model'] = 'VAR'\n",
    "results_df.loc[4, 'Mean Squared Error'] = mse\n",
    "results_df.loc[4, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[4, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[4, 'R-squared'] = r2\n",
    "\n",
    "model_fitted.save('./models/var_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ML_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('ML_model_results.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1\n",
    "q=1\n",
    "d=1\n",
    "# Fit a VARIMA model to the training data\n",
    "model = VARMAX(y_train, order=(p, d, q))\n",
    "results = model.fit()\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = results.forecast(steps=len(y_test))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[5, 'Model'] = 'VARIMA'\n",
    "results_df.loc[5, 'Mean Squared Error'] = mse\n",
    "results_df.loc[5, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[5, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[5, 'R-squared'] = r2\n",
    "\n",
    "model_fitted.save('./models/varima_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ML_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('ML_model_results.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VARMA order (p, q)\n",
    "p, q = 1, 1\n",
    "# Fit VARMA model\n",
    "model = VARMAX(y_train, order=(p, q))\n",
    "result = model.fit()\n",
    "\n",
    "# Forecast using the VARMA model\n",
    "forecast = result.forecast(steps=len(y_test))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, forecast)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, forecast)\n",
    "r2 = r2_score(y_test, forecast)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "model_fitted.save('./models/varma_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('ML_model_results.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "# Update the results in the DataFrame\n",
    "results_df.loc[6, 'Model'] = 'VARMA'\n",
    "results_df.loc[6, 'Mean Squared Error'] = mse\n",
    "results_df.loc[6, 'Root Mean Squared Error'] = rmse\n",
    "results_df.loc[6, 'Mean Absolute Error'] = mae\n",
    "results_df.loc[6, 'R-squared'] = r2\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ML_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for LSTM input\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(800, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(400, return_sequences=True))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=y_train.shape[1], activation='linear'))  # Linear activation for regression\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-20, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=1000, batch_size=4096, callbacks=[early_stopping, lr_scheduler],\n",
    "                    validation_data=(X_test_reshaped, y_test), verbose=1)\n",
    "\n",
    "# Save the entire model\n",
    "model.save(\"./models/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've trained the model and obtained predictions\n",
    "forecast = model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, forecast)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, forecast)\n",
    "r2 = r2_score(y_test, forecast)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph\n",
    "data = pd.read_csv('ML_model_results.csv')\n",
    "\n",
    "# List of models for x-axis\n",
    "models = data['Model']\n",
    "\n",
    "# Metrics for comparison\n",
    "mse = data['Mean Squared Error']\n",
    "rmse = data['Root Mean Squared Error']\n",
    "mae = data['Mean Absolute Error']\n",
    "r_squared = data['R-squared']\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(30, 24))\n",
    "\n",
    "# Mean Squared Error\n",
    "axes[0, 0].bar(models, mse, color='blue')\n",
    "axes[0, 0].set_title('Mean Squared Error')\n",
    "\n",
    "# Root Mean Squared Error\n",
    "axes[0, 1].bar(models, rmse, color='green')\n",
    "axes[0, 1].set_title('Root Mean Squared Error')\n",
    "\n",
    "# Mean Absolute Error\n",
    "axes[1, 0].bar(models, mae, color='orange')\n",
    "axes[1, 0].set_title('Mean Absolute Error')\n",
    "\n",
    "# R-squared\n",
    "axes[1, 1].bar(models, r_squared, color='red')\n",
    "axes[1, 1].set_title('R-squared')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('ML_model_results.csv')\n",
    "\n",
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Melt the DataFrame to long format for easier plotting\n",
    "melted_data = pd.melt(data, id_vars=['Model'], var_name='Metric', value_name='Value')\n",
    "\n",
    "# Create a facet grid for better visualization\n",
    "g = sns.catplot(x='Model', y='Value', hue='Metric', data=melted_data, kind='bar', height=6, aspect=5)\n",
    "\n",
    "# Set plot titles and labels\n",
    "g.fig.suptitle('Model Comparison Metrics', y=1)\n",
    "g.set(xlabel='Model', ylabel='Value')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Melt the DataFrame to long format for easier plotting\n",
    "melted_data = pd.melt(data, id_vars=['Model'], var_name='Metric', value_name='Value')\n",
    "\n",
    "# Create a grouped boxplot using Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Metric', y='Value', hue='Model', data=melted_data, palette='Set3')\n",
    "plt.title('Model Comparison Metrics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radar chart (also known as a spider chart or star plot)\n",
    "\n",
    "# List of models\n",
    "models = data['Model']\n",
    "\n",
    "# Metrics for comparison\n",
    "metrics = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-squared']\n",
    "\n",
    "# Number of metrics\n",
    "num_metrics = len(metrics)\n",
    "\n",
    "# Compute angles for each axis\n",
    "angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each model\n",
    "for i in range(len(models)):\n",
    "    values = data[metrics].iloc[i].values.tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, label=models[i])\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Model Comparison Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual vs Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your code to read and preprocess the data\n",
    "df = pd.read_csv('final_df.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df = df[['NATURAL GAS_Percent_Change', 'GOLD_Percent_Change',\n",
    "        'WTI CRUDE_Percent_Change', 'BRENT CRUDE_Percent_Change',\n",
    "        'SOYBEANS_Percent_Change', 'CORN_Percent_Change',\n",
    "        'COPPER_Percent_Change', 'ALUMINIUM_Percent_Change',\n",
    "        'ZINC_Percent_Change', 'NICKEL_Percent_Change', 'WHEAT_Percent_Change',\n",
    "        'SUGAR_Percent_Change', 'COFFEE_Percent_Change',\n",
    "        'COTTON_Percent_Change']]\n",
    "\n",
    "'''# Use vectorized operations to fill in the remaining values in 'df1'\n",
    "df1.iloc[0:, :] = (1 + df.iloc[0:, :] / 100).cumprod() * first_row.values'''\n",
    "\n",
    "df.to_csv('Actual.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models =  ['LinearRegression', 'KNeighborsRegressor', 'DecisionTreeRegressor', 'SVM']\n",
    "\n",
    "for mod in models:\n",
    "    # Load the saved model\n",
    "    model_path = f'./models/{mod}.pkl'\n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pkl.load(file)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Predict on the entire dataset (historical data)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "\n",
    "    # Assuming 'y' is your output variable names\n",
    "    df = pd.DataFrame({col.replace('_Percent_Change', ''): y_pred[:, i] for i, col in enumerate(y.columns)})\n",
    "    df.to_csv(f'{mod}_histrolic_price_prediction.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "models = ['var_model', 'varima_model', 'varima_model']\n",
    "\n",
    "# Your code to read and preprocess the data\n",
    "df = pd.read_csv('final_df.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df = df[['NATURAL GAS_Percent_Change', 'GOLD_Percent_Change',\n",
    "         'WTI CRUDE_Percent_Change', 'BRENT CRUDE_Percent_Change',\n",
    "         'SOYBEANS_Percent_Change', 'CORN_Percent_Change',\n",
    "         'COPPER_Percent_Change', 'ALUMINIUM_Percent_Change',\n",
    "         'ZINC_Percent_Change', 'NICKEL_Percent_Change', 'WHEAT_Percent_Change',\n",
    "         'SUGAR_Percent_Change', 'COFFEE_Percent_Change',\n",
    "         'COTTON_Percent_Change']]\n",
    "\n",
    "for mod in models:\n",
    "    # Load the saved model\n",
    "    model_path = f'./models/{mod}.pkl'\n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pkl.load(file)\n",
    "\n",
    "    # Forecast using the VARMA model\n",
    "    # Specify the number of steps to forecast\n",
    "    num_steps = len(df)  # You may adjust this based on your needs\n",
    "\n",
    "    # Assuming 'y' is your input variable, replace it with the appropriate variable in your dataset\n",
    "    y_pred = model.forecast(y=df.values, steps=num_steps)\n",
    "\n",
    "    # Assuming 'y' is your output variable names\n",
    "    df_result = pd.DataFrame({col.replace('_Percent_Change', ''): y_pred[:, i] for i, col in enumerate(df.columns)})\n",
    "    df_result.to_csv(f'{mod}_historical_price_prediction.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_path = f'./models/lstm.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Your code to read and preprocess the data\n",
    "df = pd.read_csv('final_df.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X = df.drop(columns=['NATURAL GAS_Percent_Change', 'GOLD_Percent_Change',\n",
    "       'WTI CRUDE_Percent_Change', 'BRENT CRUDE_Percent_Change',\n",
    "       'SOYBEANS_Percent_Change', 'CORN_Percent_Change',\n",
    "       'COPPER_Percent_Change', 'ALUMINIUM_Percent_Change',\n",
    "       'ZINC_Percent_Change', 'NICKEL_Percent_Change', 'WHEAT_Percent_Change',\n",
    "       'SUGAR_Percent_Change', 'COFFEE_Percent_Change',\n",
    "       'COTTON_Percent_Change'])\n",
    "\n",
    "y = df[['NATURAL GAS_Percent_Change', 'GOLD_Percent_Change',\n",
    "       'WTI CRUDE_Percent_Change', 'BRENT CRUDE_Percent_Change',\n",
    "       'SOYBEANS_Percent_Change', 'CORN_Percent_Change',\n",
    "       'COPPER_Percent_Change', 'ALUMINIUM_Percent_Change',\n",
    "       'ZINC_Percent_Change', 'NICKEL_Percent_Change', 'WHEAT_Percent_Change',\n",
    "       'SUGAR_Percent_Change', 'COFFEE_Percent_Change',\n",
    "       'COTTON_Percent_Change']]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])\n",
    "\n",
    "# Predict on the entire dataset (\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Assuming 'y' is your output variable names\n",
    "df = pd.DataFrame({col.replace('_Percent_Change', ''): y_pred[:, i] for i, col in enumerate(y.columns)})\n",
    "df.to_csv(f'LSTM_histrolic_price_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_stats(data, col, win):\n",
    "    rolling_mean = data[col].rolling(window=win).mean()\n",
    "    rolling_std = data[col].rolling(window=win).std()\n",
    "    historical_volatility = data[col].rolling(window=win).std() * (252 ** 0.5)\n",
    "    return rolling_mean, rolling_std, historical_volatility\n",
    "\n",
    "def feature_eng(df1, num_lags=[1, 3, 5, 7, 9, 20, 21, 100, 200], window_size=[5, 10, 20, 21, 100, 200]):\n",
    "    df2 = df1.copy()\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        for num in num_lags:\n",
    "            for lag in range(1, num + 1):\n",
    "                df2[f'{col}_Lag{lag}'] = df1[col].shift(lag)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(calculate_rolling_stats, df1, col, win) for win in window_size]\n",
    "            rolling_stats = [result.result() for result in results]\n",
    "\n",
    "        for i, win in enumerate(window_size):\n",
    "            df2[f'{col}_SMA{win}'], df2[f'{col}_STD{win}'], df2[f'{col}_HV'] = rolling_stats[i]\n",
    "\n",
    "            # Moving Averages EMA\n",
    "            df2[f'{col}_EMA{win}'] = df1[col].ewm(span=win, adjust=False).mean()\n",
    "\n",
    "        # RSI Calculation\n",
    "        delta = df1[col]\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        rs = gain / loss\n",
    "        df2[f'{col}_RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # MACD Calculation\n",
    "        df2[f'{col}_EMA12'] = df1[col].ewm(span=12, adjust=False).mean()\n",
    "        df2[f'{col}_EMA26'] = df1[col].ewm(span=26, adjust=False).mean()\n",
    "        df2[f'{col}_MACD'] = df2[f'{col}_EMA12'] - df2[f'{col}_EMA26']\n",
    "\n",
    "        # Stochastic Oscillator\n",
    "        min_val, max_val = df1[col].min(), df1[col].max()\n",
    "        df2[f'{col}_Stochastic_Oscillator'] = ((df1[col] - min_val) / (max_val - min_val)) * 100\n",
    "\n",
    "    # Get all combinations of column pairs\n",
    "    column_pairs = list(itertools.combinations(df1.columns, 2))\n",
    "\n",
    "    for col1, col2 in column_pairs:\n",
    "        # Inter-Commodity Spread Calculation:\n",
    "        df2[f'{col1}_{col2}_Spread'] = df1[col1] - df1[col2]\n",
    "\n",
    "        # Ratio Calculation between Commodities:\n",
    "        df2[f'{col1}_{col2}_Ratio'] = df1[col1] / df1[col2]\n",
    "\n",
    "    return df2\n",
    "\n",
    "# You can create placeholder values for future_data with the same columns\n",
    "\n",
    "df = pd.read_csv('df.csv')\n",
    "df = df.drop(columns=['Date'])\n",
    "df = df.dropna()\n",
    "num_future_points = 30\n",
    "future_indices = np.arange(df.index[-1] + 1, df.index[-1] + num_future_points + 1)\n",
    "\n",
    "# Create a DataFrame for future data with numerical indices\n",
    "future_data = pd.DataFrame(index=future_indices)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Load the saved model\n",
    "model_path = './models/LinearRegression.pkl'\n",
    "with open(model_path, 'rb') as file:\n",
    "    model = pkl.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you want future_data to be a DataFrame with the same columns as df\n",
    "future_data = pd.DataFrame(columns=df.columns)\n",
    "df1 = pd.DataFrame(columns=df.columns)\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "for column in df.columns:\n",
    "    df1[column] = ((df[column] - df[column].shift(1)) / df[column].shift(1)) * 100\n",
    "for i in range(num_future_points):\n",
    "\n",
    "    df1 = df1.dropna()\n",
    "    df1 = df1.iloc[-300:]\n",
    "    df2 = feature_eng(df1)\n",
    "    df2 = df2.dropna()\n",
    "    # Scale the future data using the same scaler used for historical data\n",
    "    df2 = df2.drop(columns=['NATURAL GAS', 'GOLD','WTI CRUDE', 'BRENT CRUDE',\n",
    "                                'SOYBEANS', 'CORN','COPPER', 'ALUMINIUM',\n",
    "                                'ZINC', 'NICKEL', 'WHEAT','SUGAR', 'COFFEE',\n",
    "                                'COTTON'])\n",
    "        \n",
    "\n",
    "    X_scaled_future = scaler.fit_transform(df2.values)\n",
    "\n",
    "    # Predict the next value in the future\n",
    "    y_pred_future = model.predict(X_scaled_future[-1].reshape(1, -1))\n",
    "\n",
    "\n",
    "    # Assign the predicted values to the corresponding columns in the last row\n",
    "    df.loc[df.index[-1], df.columns[:14]] = y_pred_future\n",
    "\n",
    "    # Assign the same value for 'Dollar_Index' anda 'BOND_10Y' (e.g., mean of y_pred_future)\n",
    "    common_value = np.mean(y_pred_future)  # You can use any other logic to determine this value\n",
    "    df.loc[df.index[-1], ['Dollar_Index', 'BOND_10Y']] = common_value\n",
    "    \n",
    "df = df.iloc[-num_future_points:]\n",
    "df.to_csv('LinearRegression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_stats(data, col, win):\n",
    "    rolling_mean = data[col].rolling(window=win).mean()\n",
    "    rolling_std = data[col].rolling(window=win).std()\n",
    "    historical_volatility = data[col].rolling(window=win).std() * (252 ** 0.5)\n",
    "    return rolling_mean, rolling_std, historical_volatility\n",
    "\n",
    "def feature_eng(df1, num_lags=[1, 3, 5, 7, 9, 20, 21, 100, 200], window_size=[5, 10, 20, 21, 100, 200]):\n",
    "    df2 = df1.copy()\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        for num in num_lags:\n",
    "            for lag in range(1, num + 1):\n",
    "                df2[f'{col}_Lag{lag}'] = df1[col].shift(lag)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(calculate_rolling_stats, df1, col, win) for win in window_size]\n",
    "            rolling_stats = [result.result() for result in results]\n",
    "\n",
    "        for i, win in enumerate(window_size):\n",
    "            df2[f'{col}_SMA{win}'], df2[f'{col}_STD{win}'], df2[f'{col}_HV'] = rolling_stats[i]\n",
    "\n",
    "            # Moving Averages EMA\n",
    "            df2[f'{col}_EMA{win}'] = df1[col].ewm(span=win, adjust=False).mean()\n",
    "\n",
    "        # RSI Calculation\n",
    "        delta = df1[col]\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        rs = gain / loss\n",
    "        df2[f'{col}_RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # MACD Calculation\n",
    "        df2[f'{col}_EMA12'] = df1[col].ewm(span=12, adjust=False).mean()\n",
    "        df2[f'{col}_EMA26'] = df1[col].ewm(span=26, adjust=False).mean()\n",
    "        df2[f'{col}_MACD'] = df2[f'{col}_EMA12'] - df2[f'{col}_EMA26']\n",
    "\n",
    "        # Stochastic Oscillator\n",
    "        min_val, max_val = df1[col].min(), df1[col].max()\n",
    "        df2[f'{col}_Stochastic_Oscillator'] = ((df1[col] - min_val) / (max_val - min_val)) * 100\n",
    "\n",
    "    # Get all combinations of column pairs\n",
    "    column_pairs = list(itertools.combinations(df1.columns, 2))\n",
    "\n",
    "    for col1, col2 in column_pairs:\n",
    "        # Inter-Commodity Spread Calculation:\n",
    "        df2[f'{col1}_{col2}_Spread'] = df1[col1] - df1[col2]\n",
    "\n",
    "        # Ratio Calculation between Commodities:\n",
    "        df2[f'{col1}_{col2}_Ratio'] = df1[col1] / df1[col2]\n",
    "\n",
    "    return df2\n",
    "\n",
    "# You can create placeholder values for future_data with the same columns\n",
    "\n",
    "df = pd.read_csv('df.csv')\n",
    "df = df.drop(columns=['Date'])\n",
    "df = df.dropna()\n",
    "num_future_points = 30\n",
    "future_indices = np.arange(df.index[-1] + 1, df.index[-1] + num_future_points + 1)\n",
    "\n",
    "# Create a DataFrame for future data with numerical indices\n",
    "future_data = pd.DataFrame(index=future_indices)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Load the saved model\n",
    "model_path = './models/KNeighborsRegressor.pkl'\n",
    "with open(model_path, 'rb') as file:\n",
    "    model = pkl.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you want future_data to be a DataFrame with the same columns as df\n",
    "future_data = pd.DataFrame(columns=df.columns)\n",
    "df1 = pd.DataFrame(columns=df.columns)\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "for column in df.columns:\n",
    "    df1[column] = ((df[column] - df[column].shift(1)) / df[column].shift(1)) * 100\n",
    "for i in range(num_future_points):\n",
    "\n",
    "    df1 = df1.dropna()\n",
    "    df1 = df1.iloc[-300:]\n",
    "    df2 = feature_eng(df1)\n",
    "    df2 = df2.dropna()\n",
    "    # Scale the future data using the same scaler used for historical data\n",
    "    df2 = df2.drop(columns=['NATURAL GAS', 'GOLD','WTI CRUDE', 'BRENT CRUDE',\n",
    "                                'SOYBEANS', 'CORN','COPPER', 'ALUMINIUM',\n",
    "                                'ZINC', 'NICKEL', 'WHEAT','SUGAR', 'COFFEE',\n",
    "                                'COTTON'])\n",
    "        \n",
    "\n",
    "    X_scaled_future = scaler.fit_transform(df2.values)\n",
    "\n",
    "    # Predict the next value in the future\n",
    "    y_pred_future = model.predict(X_scaled_future[-1].reshape(1, -1))\n",
    "\n",
    "\n",
    "    # Assign the predicted values to the corresponding columns in the last row\n",
    "    df.loc[df.index[-1], df.columns[:14]] = y_pred_future\n",
    "\n",
    "    # Assign the same value for 'Dollar_Index' anda 'BOND_10Y' (e.g., mean of y_pred_future)\n",
    "    common_value = np.mean(y_pred_future)  # You can use any other logic to determine this value\n",
    "    df.loc[df.index[-1], ['Dollar_Index', 'BOND_10Y']] = common_value\n",
    "    \n",
    "df = df.iloc[-num_future_points:]\n",
    "df.to_csv('knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_stats(data, col, win):\n",
    "    rolling_mean = data[col].rolling(window=win).mean()\n",
    "    rolling_std = data[col].rolling(window=win).std()\n",
    "    historical_volatility = data[col].rolling(window=win).std() * (252 ** 0.5)\n",
    "    return rolling_mean, rolling_std, historical_volatility\n",
    "\n",
    "def feature_eng(df1, num_lags=[1, 3, 5, 7, 9, 20, 21, 100, 200], window_size=[5, 10, 20, 21, 100, 200]):\n",
    "    df2 = df1.copy()\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        for num in num_lags:\n",
    "            for lag in range(1, num + 1):\n",
    "                df2[f'{col}_Lag{lag}'] = df1[col].shift(lag)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(calculate_rolling_stats, df1, col, win) for win in window_size]\n",
    "            rolling_stats = [result.result() for result in results]\n",
    "\n",
    "        for i, win in enumerate(window_size):\n",
    "            df2[f'{col}_SMA{win}'], df2[f'{col}_STD{win}'], df2[f'{col}_HV'] = rolling_stats[i]\n",
    "\n",
    "            # Moving Averages EMA\n",
    "            df2[f'{col}_EMA{win}'] = df1[col].ewm(span=win, adjust=False).mean()\n",
    "\n",
    "        # RSI Calculation\n",
    "        delta = df1[col]\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        rs = gain / loss\n",
    "        df2[f'{col}_RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # MACD Calculation\n",
    "        df2[f'{col}_EMA12'] = df1[col].ewm(span=12, adjust=False).mean()\n",
    "        df2[f'{col}_EMA26'] = df1[col].ewm(span=26, adjust=False).mean()\n",
    "        df2[f'{col}_MACD'] = df2[f'{col}_EMA12'] - df2[f'{col}_EMA26']\n",
    "\n",
    "        # Stochastic Oscillator\n",
    "        min_val, max_val = df1[col].min(), df1[col].max()\n",
    "        df2[f'{col}_Stochastic_Oscillator'] = ((df1[col] - min_val) / (max_val - min_val)) * 100\n",
    "\n",
    "    # Get all combinations of column pairs\n",
    "    column_pairs = list(itertools.combinations(df1.columns, 2))\n",
    "\n",
    "    for col1, col2 in column_pairs:\n",
    "        # Inter-Commodity Spread Calculation:\n",
    "        df2[f'{col1}_{col2}_Spread'] = df1[col1] - df1[col2]\n",
    "\n",
    "        # Ratio Calculation between Commodities:\n",
    "        df2[f'{col1}_{col2}_Ratio'] = df1[col1] / df1[col2]\n",
    "\n",
    "    return df2\n",
    "\n",
    "# You can create placeholder values for future_data with the same columns\n",
    "\n",
    "df = pd.read_csv('df.csv')\n",
    "df = df.drop(columns=['Date'])\n",
    "df = df.dropna()\n",
    "num_future_points = 30\n",
    "future_indices = np.arange(df.index[-1] + 1, df.index[-1] + num_future_points + 1)\n",
    "\n",
    "# Create a DataFrame for future data with numerical indices\n",
    "future_data = pd.DataFrame(index=future_indices)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Load the saved model\n",
    "model_path = './models/svm.pkl'\n",
    "with open(model_path, 'rb') as file:\n",
    "    model = pkl.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you want future_data to be a DataFrame with the same columns as df\n",
    "future_data = pd.DataFrame(columns=df.columns)\n",
    "df1 = pd.DataFrame(columns=df.columns)\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "for column in df.columns:\n",
    "    df1[column] = ((df[column] - df[column].shift(1)) / df[column].shift(1)) * 100\n",
    "for i in range(num_future_points):\n",
    "\n",
    "    df1 = df1.dropna()\n",
    "    df1 = df1.iloc[-300:]\n",
    "    df2 = feature_eng(df1)\n",
    "    df2 = df2.dropna()\n",
    "    # Scale the future data using the same scaler used for historical data\n",
    "    df2 = df2.drop(columns=['NATURAL GAS', 'GOLD','WTI CRUDE', 'BRENT CRUDE',\n",
    "                                'SOYBEANS', 'CORN','COPPER', 'ALUMINIUM',\n",
    "                                'ZINC', 'NICKEL', 'WHEAT','SUGAR', 'COFFEE',\n",
    "                                'COTTON'])\n",
    "        \n",
    "\n",
    "    X_scaled_future = scaler.fit_transform(df2.values)\n",
    "\n",
    "    # Predict the next value in the future\n",
    "    y_pred_future = model.predict(X_scaled_future[-1].reshape(1, -1))\n",
    "\n",
    "\n",
    "    # Assign the predicted values to the corresponding columns in the last row\n",
    "    df.loc[df.index[-1], df.columns[:14]] = y_pred_future\n",
    "\n",
    "    # Assign the same value for 'Dollar_Index' anda 'BOND_10Y' (e.g., mean of y_pred_future)\n",
    "    common_value = np.mean(y_pred_future)  # You can use any other logic to determine this value\n",
    "    df.loc[df.index[-1], ['Dollar_Index', 'BOND_10Y']] = common_value\n",
    "    \n",
    "df = df.iloc[-num_future_points:]\n",
    "df.to_csv('Svm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_stats(data, col, win):\n",
    "    rolling_mean = data[col].rolling(window=win).mean()\n",
    "    rolling_std = data[col].rolling(window=win).std()\n",
    "    historical_volatility = data[col].rolling(window=win).std() * (252 ** 0.5)\n",
    "    return rolling_mean, rolling_std, historical_volatility\n",
    "\n",
    "def feature_eng(df1, num_lags=[1, 3, 5, 7, 9, 20, 21, 100, 200], window_size=[5, 10, 20, 21, 100, 200]):\n",
    "    df2 = df1.copy()\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        for num in num_lags:\n",
    "            for lag in range(1, num + 1):\n",
    "                df2[f'{col}_Lag{lag}'] = df1[col].shift(lag)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(calculate_rolling_stats, df1, col, win) for win in window_size]\n",
    "            rolling_stats = [result.result() for result in results]\n",
    "\n",
    "        for i, win in enumerate(window_size):\n",
    "            df2[f'{col}_SMA{win}'], df2[f'{col}_STD{win}'], df2[f'{col}_HV'] = rolling_stats[i]\n",
    "\n",
    "            # Moving Averages EMA\n",
    "            df2[f'{col}_EMA{win}'] = df1[col].ewm(span=win, adjust=False).mean()\n",
    "\n",
    "        # RSI Calculation\n",
    "        delta = df1[col]\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        rs = gain / loss\n",
    "        df2[f'{col}_RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # MACD Calculation\n",
    "        df2[f'{col}_EMA12'] = df1[col].ewm(span=12, adjust=False).mean()\n",
    "        df2[f'{col}_EMA26'] = df1[col].ewm(span=26, adjust=False).mean()\n",
    "        df2[f'{col}_MACD'] = df2[f'{col}_EMA12'] - df2[f'{col}_EMA26']\n",
    "\n",
    "        # Stochastic Oscillator\n",
    "        min_val, max_val = df1[col].min(), df1[col].max()\n",
    "        df2[f'{col}_Stochastic_Oscillator'] = ((df1[col] - min_val) / (max_val - min_val)) * 100\n",
    "\n",
    "    # Get all combinations of column pairs\n",
    "    column_pairs = list(itertools.combinations(df1.columns, 2))\n",
    "\n",
    "    for col1, col2 in column_pairs:\n",
    "        # Inter-Commodity Spread Calculation:\n",
    "        df2[f'{col1}_{col2}_Spread'] = df1[col1] - df1[col2]\n",
    "\n",
    "        # Ratio Calculation between Commodities:\n",
    "        df2[f'{col1}_{col2}_Ratio'] = df1[col1] / df1[col2]\n",
    "\n",
    "    return df2\n",
    "\n",
    "# You can create placeholder values for future_data with the same columns\n",
    "\n",
    "df = pd.read_csv('df.csv')\n",
    "df = df.drop(columns=['Date'])\n",
    "df = df.dropna()\n",
    "num_future_points = 30\n",
    "future_indices = np.arange(df.index[-1] + 1, df.index[-1] + num_future_points + 1)\n",
    "\n",
    "# Create a DataFrame for future data with numerical indices\n",
    "future_data = pd.DataFrame(index=future_indices)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Load the saved model\n",
    "model_path = './models/DecisionTreeRegressor.pkl'\n",
    "with open(model_path, 'rb') as file:\n",
    "    model = pkl.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you want future_data to be a DataFrame with the same columns as df\n",
    "future_data = pd.DataFrame(columns=df.columns)\n",
    "df1 = pd.DataFrame(columns=df.columns)\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "for column in df.columns:\n",
    "    df1[column] = ((df[column] - df[column].shift(1)) / df[column].shift(1)) * 100\n",
    "for i in range(num_future_points):\n",
    "\n",
    "    df1 = df1.dropna()\n",
    "    df1 = df1.iloc[-300:]\n",
    "    df2 = feature_eng(df1)\n",
    "    df2 = df2.dropna()\n",
    "    # Scale the future data using the same scaler used for historical data\n",
    "    df2 = df2.drop(columns=['NATURAL GAS', 'GOLD','WTI CRUDE', 'BRENT CRUDE',\n",
    "                                'SOYBEANS', 'CORN','COPPER', 'ALUMINIUM',\n",
    "                                'ZINC', 'NICKEL', 'WHEAT','SUGAR', 'COFFEE',\n",
    "                                'COTTON'])\n",
    "        \n",
    "\n",
    "    X_scaled_future = scaler.fit_transform(df2.values)\n",
    "\n",
    "    # Predict the next value in the future\n",
    "    y_pred_future = model.predict(X_scaled_future[-1].reshape(1, -1))\n",
    "\n",
    "\n",
    "    # Assign the predicted values to the corresponding columns in the last row\n",
    "    df.loc[df.index[-1], df.columns[:14]] = y_pred_future\n",
    "\n",
    "    # Assign the same value for 'Dollar_Index' anda 'BOND_10Y' (e.g., mean of y_pred_future)\n",
    "    common_value = np.mean(y_pred_future)  # You can use any other logic to determine this value\n",
    "    df.loc[df.index[-1], ['Dollar_Index', 'BOND_10Y']] = common_value\n",
    "    \n",
    "df = df.iloc[-num_future_points:]\n",
    "df.to_csv('DecisionTreeRegressor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
